# Meeting Copilot

Application web de transcription et d'analyse de rÃ©unions en temps rÃ©el, alimentÃ©e par l'IA.

## FonctionnalitÃ©s

- **Transcription en temps rÃ©el** : Capture audio via microphone et transcription automatique
- **Upload de fichiers audio** : Support MP3, WAV, M4A, etc. avec transcription Whisper
- **Analyse intelligente** : DÃ©tection automatique des thÃ¨mes, dÃ©cisions et actions
- **Suggestions live** : Mise Ã  jour des suggestions toutes les 5 secondes pendant le meeting
- **Ã‰dition interactive** : Modification/suppression des suggestions en direct
- **RÃ©sumÃ© post-meeting** : GÃ©nÃ©ration d'un rapport structurÃ© et complet
- **Interface moderne** : Design responsive avec mode sombre

## Technologies

- **Frontend** : Next.js 15 (App Router) + TypeScript + TailwindCSS
- **Backend** : API Routes Next.js
- **IA** :
  - Web Speech API (transcription live via microphone)
  - OpenAI Whisper (transcription fichiers audio)
  - Claude Sonnet (analyse et rÃ©sumÃ©)
- **DÃ©ploiement** : Vercel

## Installation

### PrÃ©requis

- Node.js 18+ et npm
- Navigateur Chrome ou Edge (pour Web Speech API)
- ClÃ©s API :
  - Anthropic API Key (Claude) - Obligatoire
  - OpenAI API Key (Whisper) - Optionnel (uniquement pour upload)

### Configuration

1. Cloner le projet et installer les dÃ©pendances :

\`\`\`bash
npm install
\`\`\`

2. CrÃ©er un fichier \`.env.local\` Ã  la racine :

\`\`\`env
# Obligatoire
ANTHROPIC_API_KEY=sk-ant-...

# Optionnel (pour upload de fichiers audio)
OPENAI_API_KEY=sk-...
\`\`\`

3. Lancer le serveur de dÃ©veloppement :

\`\`\`bash
npm run dev
\`\`\`

4. Ouvrir [http://localhost:3000](http://localhost:3000)

## Utilisation

### DÃ©marrer un meeting (Live)

1. Cliquer sur "DÃ©marrer un meeting"
2. Autoriser l'accÃ¨s au microphone
3. Parler normalement - la transcription s'affiche en temps rÃ©el
4. Observer les suggestions d'IA dans le panneau de droite
5. Modifier/supprimer les suggestions si nÃ©cessaire
6. Cliquer sur "Terminer le meeting" pour gÃ©nÃ©rer le rÃ©sumÃ© final

### Uploader un enregistrement

1. Cliquer sur "Uploader un enregistrement"
2. Glisser-dÃ©poser ou sÃ©lectionner un fichier audio (MP3, WAV, etc.)
3. Attendre la transcription automatique (Whisper)
4. Consulter les suggestions gÃ©nÃ©rÃ©es automatiquement
5. GÃ©nÃ©rer le rÃ©sumÃ© final

ðŸ“– **Documentation complÃ¨te** : Voir [UPLOAD_FEATURE.md](dev_doc/UPLOAD_FEATURE.md)

### Navigation

- **/** : Page d'accueil
- **/meeting/[id]** : Page du meeting en cours
- **/summary/[id]** : Page du rÃ©sumÃ© post-meeting

## Architecture

### Structure des dossiers

\`\`\`
transcriptor/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ meeting/
â”‚   â”‚   â”‚   â”œâ”€â”€ start/      # CrÃ©ation de session
â”‚   â”‚   â”‚   â””â”€â”€ [id]/       # Get/Update meeting
â”‚   â”‚   â”œâ”€â”€ suggestions/    # Analyse live
â”‚   â”‚   â””â”€â”€ summary/        # RÃ©sumÃ© final
â”‚   â”œâ”€â”€ meeting/[id]/       # Page meeting
â”‚   â”œâ”€â”€ summary/[id]/       # Page rÃ©sumÃ©
â”‚   â””â”€â”€ page.tsx            # Page d'accueil
â”œâ”€â”€ components/
â”‚   â””â”€â”€ SuggestionsPanel.tsx
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â””â”€â”€ meeting.ts
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ claudeService.ts
â”‚       â””â”€â”€ meetingStore.ts
â””â”€â”€ doc/                    # Documentation
\`\`\`

### Flux de donnÃ©es

1. **DÃ©marrage** : \`POST /api/meeting/start\` â†’ CrÃ©e une session UUID
2. **Enregistrement** : Web Speech API â†’ Transcription continue
3. **Transcription** : \`POST /api/meeting/[id]\` â†’ Enregistre le texte
4. **Suggestions** : Polling \`POST /api/suggestions\` toutes les 5s
5. **Analyse** : Claude analyse le transcript et gÃ©nÃ¨re suggestions
6. **Fin** : \`POST /api/summary\` â†’ GÃ©nÃ¨re rÃ©sumÃ© complet
7. **Affichage** : Redirection vers \`/summary/[id]\`

### Stockage

Le MVP utilise un stockage en mÃ©moire (Map) cÃ´tÃ© serveur. Les donnÃ©es sont perdues au redÃ©marrage du serveur.

Pour un environnement de production, remplacer \`meetingStore.ts\` par une base de donnÃ©es (PostgreSQL, MongoDB, etc.).

## DÃ©ploiement sur Vercel

1. Push le code sur GitHub

2. Connecter le repository Ã  Vercel

3. Configurer les variables d'environnement :
   - \`ANTHROPIC_API_KEY\`

4. DÃ©ployer

## Limitations MVP

- **Pas de persistance** : DonnÃ©es en mÃ©moire uniquement
- **Single-user** : Pas d'authentification
- **Pas d'upload** : Fichiers audio pas encore supportÃ©s
- **Navigateurs** : Chrome/Edge uniquement (Web Speech API)
- **Langue** : FranÃ§ais par dÃ©faut

## Prochaines Ã©tapes

- [x] Upload de fichiers audio âœ… (v0.2.0)
- [ ] Persistance en base de donnÃ©es
- [ ] Diarisation (distinction speakers)
- [ ] Support multi-langues
- [ ] Authentification utilisateur
- [ ] Export PDF/Word
- [ ] Gestion de l'historique
- [ ] IntÃ©gration calendrier
- [ ] Partage de rÃ©sumÃ©s

## Documentation

Voir le dossier \`doc/\` pour plus de dÃ©tails :

- [SpÃ©cifications fonctionnelles](doc/functional_specs.md)
- [SpÃ©cifications techniques](doc/technical_specs.md)
- [Prompts Claude](doc/PROMPTS.md)
- [Variables d'environnement](doc/env_example.md)

## Support

Navigateurs supportÃ©s :
- Chrome 25+
- Edge 79+

La reconnaissance vocale nÃ©cessite une connexion HTTPS en production.

## License

MIT
